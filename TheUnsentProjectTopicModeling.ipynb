{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Unsent Project Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I looked into grouping things by colors because for LSA, it uses similarities between documents as a way to cluster words/topics and without grouping by color they documents are incredibly small and also almost random. (come back once you have better understanding of LSA to see if this is actually true, also maybe try grouping posts by time and/or color). One thing I found in the difference between using just color and using all documents, is that I found one distinct topic of hate and pain with the documents. If the colors to not really correspond significantly to the topics (which they likely don't because there are so many within each color) then it may lead to less accuracy.\n",
    "\n",
    "--- I learned that I should remove duplicates because I don't care about how often the topics are mentioned really more just that what is being talked about.\n",
    "\n",
    "--- Also I learned that I should take the lemmas of words because different forms of the word will not help to form meaningful topics\n",
    "\n",
    "--- Also because each document (mostly if they're grouped by color, I'm not entirely sure about individual posts) does not really have many significant words that characterize them, the assumption used in LSA that words can be clustered by finding that similar words appear in similar contexts (documents), this assumption is not really true, and so LSA will be less likely to be accurate/more random."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD, LatentDirichletAllocation, NMF\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('D:/Coding/Projects/Python/The Unsent Project/unsent_data_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>date</th>\n",
       "      <th>color</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Evan</td>\n",
       "      <td>June 30 2020</td>\n",
       "      <td>Tangerine</td>\n",
       "      <td>All I want is you to be happy. I love you fore...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Procureur</td>\n",
       "      <td>June 30 2020</td>\n",
       "      <td>Light Pink</td>\n",
       "      <td>Tu es mon meilleur ami</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>shiv</td>\n",
       "      <td>June 30 2020</td>\n",
       "      <td>Pale Purple</td>\n",
       "      <td>i loved you... more than you ever cared for me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>brendan</td>\n",
       "      <td>June 30 2020</td>\n",
       "      <td>Yellow</td>\n",
       "      <td>you make me so incredibly happy &amp; im so in lov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>James</td>\n",
       "      <td>June 30 2020</td>\n",
       "      <td>Yellow</td>\n",
       "      <td>There this thing about you. No matter how many...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        name          date        color  \\\n",
       "0       Evan  June 30 2020    Tangerine   \n",
       "1  Procureur  June 30 2020   Light Pink   \n",
       "2       shiv  June 30 2020  Pale Purple   \n",
       "3    brendan  June 30 2020       Yellow   \n",
       "4      James  June 30 2020       Yellow   \n",
       "\n",
       "                                             message  \n",
       "0  All I want is you to be happy. I love you fore...  \n",
       "1                             Tu es mon meilleur ami  \n",
       "2  i loved you... more than you ever cared for me...  \n",
       "3  you make me so incredibly happy & im so in lov...  \n",
       "4  There this thing about you. No matter how many...  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization\n",
    "For uncovering meaningful topics, I initially thought that different forms of words would be unhelpful, but different tenses do indicate different meanings in the topics, so I've chose to leave the tokens as they are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop words\n",
    "A lot of stop words like you, him, her, more, them, want, etc. can all be somewhat characterizing of certain topics in this situation, so I'm choosing to make my own list mostly of articles, prepositions, and conjunctions, part of speeches that don't really make a big difference towards the topics. I mostly gathered these words from initial runs of topic modeling with stop words, and pulled the words I found to be less helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ['to', 'and', 'the', 'a', 'for', 'but', 'so', 'in', 'of', 'it', \\\n",
    "             'about', 'as', 'this', 'do', 'doing', 'did', 'be', 'on', 'off', \\\n",
    "             'with', 'that', 'this', 'been', 'just', 'at']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 10\n",
    "n_words = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topic_words(model, vocab):\n",
    "    for i, component in enumerate(model.components_):\n",
    "        terms_comp = zip(vocab, component)\n",
    "        sorted_terms = sorted(terms_comp, key = lambda x: x[1], reverse=True)[:n_words]\n",
    "        print(\"Topic \" + str(i), ' '.join([term for term, comp in sorted_terms]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSA - Latent Semantic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
    "documents = df['message'].apply(lambda x: str(x))#use_idf=True, smooth_idf=True\n",
    "vectors = vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(199931, 32857)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa_model = TruncatedSVD(n_components=n_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=10, n_iter=5,\n",
       "             random_state=None, tol=0.0)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa_model.fit(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 you me love my still miss know wish we never\n",
      "Topic 1 love always will still first you my much forever ll\n",
      "Topic 2 me you loved why hurt feel made like hate broke\n",
      "Topic 3 miss you much still everyday hate come lot talking sometimes\n",
      "Topic 4 my heart first were friend best life you always broke\n",
      "Topic 5 don know think still want re can if hope anymore\n",
      "Topic 6 still think me my were when broke time all every\n",
      "Topic 7 sorry was im enough not wasn love why good really\n",
      "Topic 8 always will sorry me still never ll we think miss\n",
      "Topic 9 sorry think much still loved you how never wish will\n"
     ]
    }
   ],
   "source": [
    "display_topic_words(lsa_model, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa_model.singular_values_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 you me love my still miss know wish we never\n",
      "Topic 1 love always will first you still my much forever ll\n",
      "Topic 2 me you loved why hurt hate much made feel broke\n",
      "Topic 3 miss you much still everyday hope come lot sometimes talking\n",
      "Topic 4 my heart first were friend best life always miss broke\n",
      "Topic 5 don know think still want if re can anymore even\n",
      "Topic 6 still think were me my we first time re broke\n",
      "Topic 7 sorry was enough im why love wasn not were good\n",
      "Topic 8 sorry always still will think never was enough im ll\n",
      "Topic 9 me we were miss love why like what first made\n"
     ]
    }
   ],
   "source": [
    "display_topic_words(svd_model, vocab) # this is displaying the previous svd model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA - Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words=stop_words)\n",
    "documents = df['message'].apply(lambda x: str(x))\n",
    "vectors = vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LatentDirichletAllocation(n_components=n_topics, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "                          evaluate_every=-1, learning_decay=0.7,\n",
       "                          learning_method='batch', learning_offset=10.0,\n",
       "                          max_doc_update_iter=100, max_iter=10,\n",
       "                          mean_change_tol=0.001, n_components=10, n_jobs=None,\n",
       "                          perp_tol=0.1, random_state=1, topic_word_prior=None,\n",
       "                          total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.fit(vectors) # this takes a little while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 you dont know we im want like if what miss\n",
      "Topic 1 you my heart your me when at back all broke\n",
      "Topic 2 you me love much still how dont never hate will\n",
      "Topic 3 my you were life first youre best love person friend\n",
      "Topic 4 you we always will love ill our years its together\n",
      "Topic 5 me fuck thank you ur bitch fucking up shit youre\n",
      "Topic 6 your you im miss sorry still me my its not\n",
      "Topic 7 hope you youre really good are happy out we enough\n",
      "Topic 8 you me was more love loved never than im wish\n",
      "Topic 9 you me wish would if was have didnt when could\n"
     ]
    }
   ],
   "source": [
    "display_topic_words(lda_model, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMF - Non-negative Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words=stop_words)\n",
    "documents = df['message'].apply(lambda x: str(x))\n",
    "vectors = vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_model = NMF(n_components=n_topics, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init=None, l1_ratio=0.0, max_iter=200,\n",
       "    n_components=10, random_state=1, shuffle=False, solver='cd', tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf_model.fit(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 you still much are loved hope were hate miss think\n",
      "Topic 1 me like made feel why hurt make how when what\n",
      "Topic 2 we were still together had other each friends miss when\n",
      "Topic 3 my heart life were first best friend youre all is\n",
      "Topic 4 love always will still first much you forever ill thank\n",
      "Topic 5 im sorry not youre still happy now its scared like\n",
      "Topic 6 know dont want if like think what its how even\n",
      "Topic 7 was when like what were all didnt because had one\n",
      "Topic 8 your miss hope is still its when youre all smile\n",
      "Topic 9 have never wish could will how always would much back\n"
     ]
    }
   ],
   "source": [
    "display_topic_words(nmf_model, vocab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
